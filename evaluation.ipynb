{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import funcs \n",
    "import load_data\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import subprocess\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:25:00.0, compute capability: 7.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.compat.v1.ConfigProto(device_count={\"GPU\":1, \"CPU\": 10})\n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a ssh-tunnel to server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'ssh -N -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org &'\n",
    "ssh_session = subprocess.Popen('exec ' + command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "\n",
    "# Creating/Setting the experiment\n",
    "experiment_name = '/chexpert_label_interdependence'\n",
    "\n",
    "# Line below should be commented if the experiment is already created\n",
    "# If kept commented during the first run of a new experiment, the set_experiment \n",
    "# will automatically create the new experiment with local artifact storage\n",
    "\n",
    "# mlflow.create_experiment(name=experiment_name, artifact_location=artifact)\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "\n",
    "# Loading the optimization parameters aturomatically from keras\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# Starting the MLflow \n",
    "mlflow.start_run(run_name = 'Uncertainty Measurement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Git commit  (only in Jupyter notebook)\n",
    "This is only needed for jupyter notebook\n",
    "\n",
    "You can annotate runs with arbitrary tags. Tag keys that start with mlflow. are reserved for internal use. The following tags are set automatically by MLflow, when appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git commit hash a19a8e09ea5119cbe4276afc4b3b73a1ae0bc20d\n"
     ]
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_commit_hash = repo.head.object.hexsha\n",
    "print('git commit hash', git_commit_hash)\n",
    "\n",
    "mlflow.set_tag('mlflow.source.git.commit', git_commit_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# epochs, batch_size, max_sample = funcs.reading_terminal_inputs()\n",
    "epochs, batch_size, max_sample = 3, 32, 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'chexpert' # 'nih'\n",
    "dir = '/groups/jjrodrig/projects/chest/dataset/' + dataset + '/'\n",
    "\n",
    "mlflow.log_param('dataset',dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sample-pruning\n",
      "train: (223414, 19)\n",
      "test: (234, 19)\n",
      "\n",
      "after sample-pruning\n",
      "train (certain): (511, 20)\n",
      "train (uncertain): (361, 20)\n",
      "valid: (128, 20)\n",
      "test: (169, 20) \n",
      "\n",
      "Found 511 validated image filenames.\n",
      "Found 128 validated image filenames.\n",
      "CPU times: user 1.89 s, sys: 192 ms, total: 2.08 s\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(train_dataset, valid_dataset), (train_generator, valid_generator), Info= load_data.load(dir=dir, dataset=dataset, batch_size=30, mode='train_val', max_sample=max_sample)\n",
    "\n",
    "mlflow.log_param('train count',len(train_generator.filenames))\n",
    "mlflow.log_param('valid count',len(valid_generator.filenames))\n",
    "mlflow.log_param('max_sample',max_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'chexpert' # 'nih'\n",
    "# dir = '/groups/jjrodrig/projects/chest/dataset/' + dataset + '/'\n",
    "\n",
    "\n",
    "# # Loading the data\n",
    "# max_sample = 1000\n",
    "# test_generator, Info = load_data.load(dir=dir, dataset='chexpert', batch_size=30, mode='test', max_sample=max_sample)\n",
    "\n",
    "# # Loading the model\n",
    "# model = tf.keras.models.load_model(dir + 'model/model.h5')\n",
    "\n",
    "# # Compiling the model\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=funcs.weighted_bce_loss(Info.class_weights), metrics=[tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Measuring loss & Accuracy for all test samples (average over all classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test, y_test = next(test_generator)\n",
    "# L = len(test_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# score = {}\n",
    "# NUM_CLASSES = y_test.shape[1]\n",
    "\n",
    "# for ix in tqdm(range(L)):\n",
    "    \n",
    "#     name, x,y = test_generator.filenames[ix] , x_test[ix,...] , y_test[ix,...]\n",
    "#     x,y = x[np.newaxis,:] , y[np.newaxis,:]\n",
    "    \n",
    "#     # Estimating the loss & accuracy for instance\n",
    "#     eval = model.evaluate(x=x, y=y,verbose=0)\n",
    "\n",
    "#     # predicting the labels for instance\n",
    "#     pred = model.predict(x=x,verbose=0)\n",
    "\n",
    "#     # Measuring the loss for each class\n",
    "#     loss_per_class = [ tf.keras.losses.binary_crossentropy(y[...,d],pred[...,d]) for d in range(NUM_CLASSES)]\n",
    "\n",
    "#     # saving all the infos\n",
    "#     score[name] = {'filenames':test_generator.filenames,'loss_avg':eval[0], 'acc_avg':eval[1], 'predictions':pred, 'truth':y_test, 'loss':np.array(loss_per_class)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the outputs to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(score).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as mlflow artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = str(int(time()))\n",
    "# df.to_json(dir + 'model/test_results_'+tm+'.json')\n",
    "mlflow.log_artifact(dir + 'model/test_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the mlflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLosing the ssh session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the results:\n",
    "\n",
    "        \n",
    "    >> ssh -N -L 5000:localhost:5432 artinmajdi@data7-db1.cyverse.org &\n",
    "    >> mlflow ui --backend-store-uri postgresql://artinmajdi:1234@localhost:5000/chest_db --port 6789             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('mlflow-xray': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0dd998473dbd4892f34807bf19aeea9c12a70ba84b1a5d02d168816cec7a7d398"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
