{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import funcs \n",
    "import load_data\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import subprocess\n",
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.compat.v1.ConfigProto(device_count={\"GPU\":1, \"CPU\": 10})\n",
    "config.gpu_options.allow_growth = True  \n",
    "config.log_device_placement = True  \n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "server, artifact = funcs.mlflow_settings()\n",
    "mlflow.set_tracking_uri(server)\n",
    "\n",
    "\n",
    "# Creating/Setting the experiment\n",
    "experiment_name = '/chexpert_d1'\n",
    "\n",
    "# Line below should be commented if the experiment is already created\n",
    "# If kept commented during the first run of a new experiment, the set_experiment \n",
    "# will automatically create the new experiment with local artifact storage\n",
    "\n",
    "mlflow.create_experiment(name=experiment_name, artifact_location=artifact)\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "\n",
    "# Loading the optimization parameters aturomatically from keras\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "# Starting the MLflow \n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a ssh-tunnel to server in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = 'ssh -N -L 5000:localhost:5432 <username>@<remote-server-address> &'\n",
    "ssh_session = subprocess.Popen('exec ' + command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Terminal Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs, batch_size = funcs.reading_terminal_inputs()\n",
    "epochs, batch_size = 3, 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'chexpert' # 'nih'\n",
    "dir = '/groups/jjrodrig/projects/chest/dataset/' + dataset + '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Git commit  (only in Jupyter notebook)\n",
    "This is only needed for jupyter notebook\n",
    "\n",
    "You can annotate runs with arbitrary tags. Tag keys that start with mlflow. are reserved for internal use. The following tags are set automatically by MLflow, when appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git commit hash 90d4abf577a3b4038a811d89bd87df3b4a7ae707\n"
     ]
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_commit_hash = repo.head.object.hexsha\n",
    "print('git commit hash', git_commit_hash)\n",
    "\n",
    "mlflow.set_tag('mlflow.source.git.commit', git_commit_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "train size: (478, 23)\n",
      "valid size: (120, 23)\n",
      "test size: (202, 23)\n",
      "Found 478 validated image filenames.\n",
      "Found 120 validated image filenames.\n",
      "Found 202 validated image filenames.\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n",
      "Epoch 1/3\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  if not isinstance(values, collections.Sequence):\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/tensorflow/python/training/tracking/data_structures.py:718: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  if not isinstance(wrapped_dict, collections.Mapping):\n",
      "15/15 [==============================] - 42s 3s/step - loss: 0.0252 - binary_accuracy: 0.8583 - val_loss: 0.1224 - val_binary_accuracy: 0.8274\n",
      "Epoch 2/3\n",
      "15/15 [==============================] - 39s 3s/step - loss: 0.0200 - binary_accuracy: 0.8973 - val_loss: 0.2094 - val_binary_accuracy: 0.7988\n",
      "Epoch 3/3\n",
      "15/15 [==============================] - 40s 3s/step - loss: 0.0172 - binary_accuracy: 0.9062 - val_loss: 0.1958 - val_binary_accuracy: 0.6524\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset, Info = load_data.load(dir=dir, dataset=dataset, batch_size=30, mode='train_val')\n",
    "\n",
    "funcs.optimize(dir, train_dataset, valid_dataset, epochs, Info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the mlflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLosing the ssh session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_session.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "train size: (61470, 23)\n",
      "valid size: (15367, 23)\n",
      "test size: (23163, 23)\n",
      "Found 23163 validated image filenames.\n",
      "/home/u29/mohammadsmajdi/anaconda3/envs/mlflow-xray/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "dir = '/groups/jjrodrig/projects/chest/dataset/nih/'\n",
    "\n",
    "# Loading the data\n",
    "test_generator, Info = load_data.load(dir=dir, dataset='nih', batch_size=30, mode='test')\n",
    "\n",
    "# Loading the model\n",
    "model = tf.keras.models.load_model(dir + 'model/model.h5')\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=funcs.weighted_bce_loss(Info.class_weights), metrics=[tf.keras.metrics.binary_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring loss & Accuracy for all test samples (average over all classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "score = {}\n",
    "NUM_CLASSES = 14\n",
    "\n",
    "for name in tqdm(test_generator.filenames): \n",
    "\n",
    "    x_test, y_test = next(test_generator)\n",
    "\n",
    "    # Estimating the loss & accuracy for instance\n",
    "    eval = model.evaluate(x=x_test,y=y_test,verbose=0)\n",
    "\n",
    "    # predicting the labels for instance\n",
    "    pred = model.predict(x=x_test,verbose=0)\n",
    "\n",
    "    # Measuring the loss for each class\n",
    "    loss_per_class = [ tf.keras.losses.binary_crossentropy(y_test[...,d],pred[...,d]) for d in range(NUM_CLASSES)]\n",
    "\n",
    "    # saving all the infos\n",
    "    score[name] = {'loss_avg':eval[0], 'acc_avg':eval[1], 'predictions':pred, 'truth':y_test, 'loss':np.array(loss_per_class)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the outputs to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(score).T\n",
    "\n",
    "# saving the dataframe as csv file to add to mlflow as an artifact\n",
    "df.to_json(dir + 'model/test_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ssh tunneling:\n",
    "\n",
    "- Step 1 (before running the code): Connecting to remote server through ssh tunneling\n",
    "        \n",
    "        ssh -L 5000:localhost:5432 <username>@<remote-server-address>\n",
    "\n",
    "- Step 2 (after running the code): Connecting to remote postgres server\n",
    "        \n",
    "        mlflow ui --backend-store-uri postgresql://<postgres-username>:<pass>@localhost:5000/<database> --port 6789             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd09d8d807799ff101ff985f0c408abccad7d9f7b84209af85c18a27a5a640409ad",
   "display_name": "Python 3.7.9 64-bit ('mlflow': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}